import os
import json
import time
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

import faiss
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import tiktoken

from django.conf import settings
from django.core.cache import cache

from documents.models import Document, DocumentChunk
from .models import VectorStore, SearchQuery, SearchResult
from .embedding_cache import embedding_cache

class EmbeddingService:
    """Servi√ßo para gerar embeddings otimizado para portugu√™s"""
    
    def __init__(self):
        self.model_name = settings.EMBEDDING_MODEL
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """Carrega modelo de embedding com cache"""
        cache_key = f"embedding_model_{self.model_name}"
        self.model = cache.get(cache_key)
        
        if self.model is None:
            self.model = SentenceTransformer(self.model_name)
            # Cache por 1 hora
            cache.set(cache_key, self.model, 3600)
    
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """Gera embeddings para lista de textos com cache"""
        if not texts:
            return np.array([])
        
        # Normalizar textos
        normalized_texts = [self._preprocess_text(text) for text in texts]
        
        # Buscar embeddings no cache
        cached_embeddings, missing_texts = embedding_cache.get_multiple_embeddings(
            normalized_texts, 
            model_name=self.model_name
        )
        
        # Gerar embeddings apenas para textos n√£o encontrados no cache
        if missing_texts:
            print(f"üîÑ Gerando {len(missing_texts)} novos embeddings (cache miss)")
            new_embeddings = self.model.encode(
                missing_texts,
                batch_size=32,
                show_progress_bar=len(missing_texts) > 50,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            
            # Salvar novos embeddings no cache
            text_embedding_pairs = list(zip(missing_texts, new_embeddings))
            saved_count = embedding_cache.save_multiple_embeddings(
                text_embedding_pairs, 
                model_name=self.model_name
            )
            print(f"üíæ Salvos {saved_count}/{len(missing_texts)} embeddings no cache")
        else:
            print(f"üéØ Todos os {len(texts)} embeddings encontrados no cache!")
            new_embeddings = np.array([])
        
        # Combinar embeddings cached e novos
        result_embeddings = []
        new_idx = 0
        
        for i, cached_emb in enumerate(cached_embeddings):
            if cached_emb is not None:
                result_embeddings.append(cached_emb)
            else:
                result_embeddings.append(new_embeddings[new_idx])
                new_idx += 1
        
        return np.array(result_embeddings)
    
    def encode_single_text(self, text: str) -> np.ndarray:
        """Gera embedding para um √∫nico texto com cache"""
        normalized_text = self._preprocess_text(text)
        
        # Tentar buscar no cache primeiro
        cached_embedding = embedding_cache.get_embedding(
            normalized_text, 
            model_name=self.model_name
        )
        
        if cached_embedding is not None:
            print(f"üéØ Embedding encontrado no cache")
            return cached_embedding
        
        # Cache miss - gerar novo embedding
        print(f"üîÑ Gerando novo embedding (cache miss)")
        embedding = self.model.encode(
            [normalized_text],
            convert_to_numpy=True,
            normalize_embeddings=True
        )[0]
        
        # Salvar no cache
        if embedding_cache.save_embedding(normalized_text, embedding, model_name=self.model_name):
            print(f"üíæ Embedding salvo no cache")
        
        return embedding
    
    def _preprocess_text(self, text: str) -> str:
        """Pr√©-processa texto para melhor qualidade dos embeddings"""
        # Remover quebras de linha excessivas
        text = text.replace('\n\n\n', '\n\n')
        text = text.replace('\r\n', '\n')
        
        # Limitar tamanho (modelos t√™m limite de tokens)
        if len(text) > 8000:  # Aproximadamente 2000 tokens
            text = text[:8000]
        
        return text.strip()
    
    def get_dimension(self) -> int:
        """Retorna dimens√£o dos embeddings do modelo atual"""
        if self.model is None:
            self._load_model()
        return self.model.get_sentence_embedding_dimension()
    
    def generate_embeddings_for_document(self, document: Document):
        """Gera embeddings para todos os chunks de um documento"""
        chunks = document.chunks.all().order_by('chunk_index')
        
        if not chunks:
            return
        
        # Extrair textos dos chunks
        texts = [chunk.content for chunk in chunks]
        
        # Gerar embeddings
        embeddings = self.encode_texts(texts)
        
        # Salvar embeddings nos chunks
        for chunk, embedding in zip(chunks, embeddings):
            chunk.embedding = embedding.tolist()
            chunk.save(update_fields=['embedding'])

class ChunkingService:
    """Servi√ßo para divis√£o de documentos em chunks otimizado para portugu√™s"""
    
    def __init__(self):
        self.tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
    
    def create_chunks(
        self, 
        text: str, 
        chunk_size: int = 700, 
        overlap: int = 100,
        preserve_structure: bool = True
    ) -> List[Dict[str, Any]]:
        """Cria chunks de texto com sobreposi√ß√£o"""
        
        if not text.strip():
            return []
        
        # Dividir por estrutura se preservar estrutura estiver ativo
        if preserve_structure:
            return self._chunk_by_structure(text, chunk_size, overlap)
        else:
            return self._chunk_by_size(text, chunk_size, overlap)
    
    def _chunk_by_structure(self, text: str, chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
        """Divide texto preservando estrutura (cabe√ßalhos, par√°grafos)"""
        chunks = []
        
        # Dividir por se√ß√µes (cabe√ßalhos markdown)
        sections = self._split_by_headers(text)
        
        current_chunk = ""
        current_size = 0
        chunk_index = 0
        
        for section in sections:
            section_tokens = self._count_tokens(section['content'])
            
            # Se a se√ß√£o cabe no chunk atual
            if current_size + section_tokens <= chunk_size:
                current_chunk += section['content'] + "\n\n"
                current_size += section_tokens
            else:
                # Salvar chunk atual se n√£o estiver vazio
                if current_chunk.strip():
                    chunks.append({
                        'content': current_chunk.strip(),
                        'chunk_index': chunk_index,
                        'start': len(current_chunk) - len(current_chunk.lstrip()),
                        'end': len(current_chunk),
                        'section_title': section.get('title', ''),
                        'tokens': current_size
                    })
                    chunk_index += 1
                
                # Se a se√ß√£o √© muito grande, dividir por par√°grafos
                if section_tokens > chunk_size:
                    section_chunks = self._chunk_large_section(
                        section['content'], 
                        chunk_size, 
                        overlap,
                        chunk_index
                    )
                    chunks.extend(section_chunks)
                    chunk_index += len(section_chunks)
                    current_chunk = ""
                    current_size = 0
                else:
                    # Iniciar novo chunk com esta se√ß√£o
                    current_chunk = section['content'] + "\n\n"
                    current_size = section_tokens
        
        # Adicionar √∫ltimo chunk se n√£o estiver vazio
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'chunk_index': chunk_index,
                'start': 0,
                'end': len(current_chunk),
                'tokens': current_size
            })
        
        return chunks
    
    def _chunk_by_size(self, text: str, chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
        """Divide texto por tamanho fixo com sobreposi√ß√£o"""
        chunks = []
        tokens = self.tokenizer.encode(text)
        
        start = 0
        chunk_index = 0
        
        while start < len(tokens):
            # Definir fim do chunk
            end = min(start + chunk_size, len(tokens))
            
            # Extrair tokens do chunk
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.decode(chunk_tokens)
            
            chunks.append({
                'content': chunk_text,
                'chunk_index': chunk_index,
                'start': start,
                'end': end,
                'tokens': len(chunk_tokens)
            })
            
            # Pr√≥ximo chunk com sobreposi√ß√£o
            start = end - overlap
            chunk_index += 1
            
            # Evitar chunks muito pequenos no final
            if len(tokens) - start < overlap:
                break
        
        return chunks
    
    def _split_by_headers(self, text: str) -> List[Dict[str, str]]:
        """Divide texto por cabe√ßalhos markdown"""
        lines = text.split('\n')
        sections = []
        current_section = []
        current_title = ""
        
        for line in lines:
            if line.strip().startswith('#'):
                # Salvar se√ß√£o anterior
                if current_section:
                    sections.append({
                        'title': current_title,
                        'content': '\n'.join(current_section)
                    })
                
                # Iniciar nova se√ß√£o
                current_title = line.strip()
                current_section = [line]
            else:
                current_section.append(line)
        
        # Adicionar √∫ltima se√ß√£o
        if current_section:
            sections.append({
                'title': current_title,
                'content': '\n'.join(current_section)
            })
        
        return sections
    
    def _chunk_large_section(
        self, 
        text: str, 
        chunk_size: int, 
        overlap: int, 
        start_index: int
    ) -> List[Dict[str, Any]]:
        """Divide se√ß√£o grande em chunks menores"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        current_size = 0
        chunk_index = start_index
        
        for paragraph in paragraphs:
            paragraph_tokens = self._count_tokens(paragraph)
            
            if current_size + paragraph_tokens <= chunk_size:
                current_chunk += paragraph + "\n\n"
                current_size += paragraph_tokens
            else:
                # Salvar chunk atual
                if current_chunk.strip():
                    chunks.append({
                        'content': current_chunk.strip(),
                        'chunk_index': chunk_index,
                        'tokens': current_size
                    })
                    chunk_index += 1
                
                # Iniciar novo chunk
                current_chunk = paragraph + "\n\n"
                current_size = paragraph_tokens
        
        # Adicionar √∫ltimo chunk
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'chunk_index': chunk_index,
                'tokens': current_size
            })
        
        return chunks
    
    def _count_tokens(self, text: str) -> int:
        """Conta tokens no texto"""
        return len(self.tokenizer.encode(text))

class VectorSearchService:
    """Servi√ßo de busca vetorial usando FAISS"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.vector_store = None
        self.document_chunks = {}
        self._load_vector_store()
    
    def _load_vector_store(self):
        """Carrega ou cria vector store"""
        try:
            vector_store_config = VectorStore.objects.filter(is_active=True).first()
            
            if vector_store_config and os.path.exists(vector_store_config.index_path):
                # Carregar √≠ndice existente
                self.vector_store = faiss.read_index(vector_store_config.index_path)
                self._load_chunk_mapping()
            else:
                # Criar novo √≠ndice
                self._create_new_index()
                
        except Exception as e:
            print(f"Erro ao carregar vector store: {e}")
            self._create_new_index()
    
    def _create_new_index(self):
        """Cria novo √≠ndice FAISS"""
        dimension = self.embedding_service.get_dimension()
        
        # Usar HNSW para melhor performance em buscas
        self.vector_store = faiss.IndexHNSWFlat(dimension, 32)
        self.vector_store.hnsw.efConstruction = 200
        self.vector_store.hnsw.efSearch = 50
        
        self.document_chunks = {}
    
    def _load_chunk_mapping(self):
        """Carrega mapeamento de chunks"""
        cache_key = "vector_store_chunk_mapping"
        self.document_chunks = cache.get(cache_key, {})
        
        if not self.document_chunks:
            # Recriar mapeamento do banco
            chunks = DocumentChunk.objects.filter(
                embedding__isnull=False
            ).select_related('document')
            
            for i, chunk in enumerate(chunks):
                self.document_chunks[i] = {
                    'document_id': chunk.document.id,
                    'chunk_id': chunk.id,
                    'chunk_index': chunk.chunk_index,
                    'content': chunk.content
                }
            
            # Cache por 1 hora
            cache.set(cache_key, self.document_chunks, 3600)
    
    def add_document_embeddings(self, document: Document):
        """Adiciona embeddings de um documento ao √≠ndice"""
        chunks = document.chunks.filter(embedding__isnull=False)
        
        if not chunks:
            return
        
        # Converter embeddings para numpy array
        embeddings = []
        for chunk in chunks:
            embeddings.append(np.array(chunk.embedding))
        
        embeddings_array = np.vstack(embeddings)
        
        # Adicionar ao √≠ndice
        start_idx = self.vector_store.ntotal
        self.vector_store.add(embeddings_array)
        
        # Atualizar mapeamento de chunks
        for i, chunk in enumerate(chunks):
            self.document_chunks[start_idx + i] = {
                'document_id': chunk.document.id,
                'chunk_id': chunk.id,
                'chunk_index': chunk.chunk_index,
                'content': chunk.content
            }
        
        # Salvar √≠ndice atualizado
        self._save_vector_store()
    
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Busca sem√¢ntica por similaridade"""
        if not query.strip() or self.vector_store.ntotal == 0:
            return []
        
        # Gerar embedding da query
        query_embedding = self.embedding_service.encode_single_text(query)
        query_vector = query_embedding.reshape(1, -1)
        
        # Buscar no √≠ndice
        scores, indices = self.vector_store.search(query_vector, k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx in self.document_chunks:
                chunk_info = self.document_chunks[idx]
                results.append({
                    'document_id': chunk_info['document_id'],
                    'chunk_id': chunk_info['chunk_id'],
                    'chunk_index': chunk_info['chunk_index'],
                    'content': chunk_info['content'],
                    'score': float(score),
                    'search_type': 'semantic'
                })
        
        return results
    
    def _save_vector_store(self):
        """Salva √≠ndice FAISS em disco"""
        try:
            index_path = os.path.join(settings.VECTOR_STORE_PATH, 'faiss_index.bin')
            os.makedirs(os.path.dirname(index_path), exist_ok=True)
            
            faiss.write_index(self.vector_store, index_path)
            
            # Atualizar configura√ß√£o
            vector_store_config, created = VectorStore.objects.get_or_create(
                name='default',
                defaults={
                    'embedding_model': settings.EMBEDDING_MODEL,
                    'dimension': self.embedding_service.get_dimension(),
                    'index_path': index_path,
                    'vectors_count': self.vector_store.ntotal
                }
            )
            
            if not created:
                vector_store_config.vectors_count = self.vector_store.ntotal
                vector_store_config.save()
            
            # Atualizar cache
            cache.set("vector_store_chunk_mapping", self.document_chunks, 3600)
            
        except Exception as e:
            print(f"Erro ao salvar vector store: {e}")

class BM25SearchService:
    """Servi√ßo de busca BM25 (keyword-based)"""
    
    def __init__(self):
        self.bm25_index = None
        self.document_chunks = []
        self._load_bm25_index()
    
    def _load_bm25_index(self):
        """Carrega ou cria √≠ndice BM25"""
        cache_key = "bm25_index"
        cached_data = cache.get(cache_key)
        
        if cached_data:
            self.bm25_index = cached_data['index']
            self.document_chunks = cached_data['chunks']
        else:
            self._create_bm25_index()
    
    def _create_bm25_index(self):
        """Cria √≠ndice BM25 com todos os chunks"""
        chunks = DocumentChunk.objects.select_related('document').all()
        
        self.document_chunks = []
        corpus = []
        
        for chunk in chunks:
            self.document_chunks.append({
                'document_id': chunk.document.id,
                'chunk_id': chunk.id,
                'chunk_index': chunk.chunk_index,
                'content': chunk.content
            })
            
            # Tokenizar para BM25 (portugu√™s)
            tokens = self._tokenize_portuguese(chunk.content)
            corpus.append(tokens)
        
        if corpus:
            self.bm25_index = BM25Okapi(corpus)
        
        # Cache por 1 hora
        cache.set("bm25_index", {
            'index': self.bm25_index,
            'chunks': self.document_chunks
        }, 3600)
    
    def _tokenize_portuguese(self, text: str) -> List[str]:
        """Tokeniza√ß√£o otimizada para portugu√™s"""
        import re
        
        # Converter para min√∫sculas
        text = text.lower()
        
        # Remover pontua√ß√£o e caracteres especiais
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Dividir em tokens
        tokens = text.split()
        
        # Filtrar tokens muito curtos (stopwords b√°sicas)
        stop_words = {
            'a', 'ao', 'aos', 'as', 'da', 'das', 'de', 'do', 'dos', 'e', 'em', 'na', 'nas', 'no', 'nos',
            'o', 'os', 'ou', 'para', 'por', 'que', 'se', 'um', 'uma', 'mas', 'como', 'com', '√©', 's√£o',
            'foi', 'ser', 'ter', 'sua', 'seu', 'seus', 'suas', 'ele', 'ela', 'eles', 'elas', 'isso', 'isto'
        }
        
        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        
        return tokens
    
    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Busca BM25"""
        if not query.strip() or not self.bm25_index:
            return []
        
        # Tokenizar query
        query_tokens = self._tokenize_portuguese(query)
        
        if not query_tokens:
            return []
        
        # Buscar
        scores = self.bm25_index.get_scores(query_tokens)
        
        # Ordenar por score
        top_indices = np.argsort(scores)[::-1][:k]
        
        results = []
        for idx in top_indices:
            if idx < len(self.document_chunks) and scores[idx] > 0:
                chunk_info = self.document_chunks[idx]
                results.append({
                    'document_id': chunk_info['document_id'],
                    'chunk_id': chunk_info['chunk_id'],
                    'chunk_index': chunk_info['chunk_index'],
                    'content': chunk_info['content'],
                    'score': float(scores[idx]),
                    'search_type': 'bm25'
                })
        
        return results
    
    def update_index(self):
        """Atualiza √≠ndice BM25"""
        self._create_bm25_index()

class HybridSearchService:
    """Servi√ßo de busca h√≠brida combinando busca sem√¢ntica e BM25"""
    
    def __init__(self):
        self.vector_search = VectorSearchService()
        self.bm25_search = BM25SearchService()
        self.semantic_weight = settings.SEMANTIC_WEIGHT
        self.bm25_weight = settings.BM25_WEIGHT
    
    def search(
        self, 
        query: str, 
        k: int = 5,
        semantic_weight: Optional[float] = None,
        bm25_weight: Optional[float] = None,
        user: Any = None
    ) -> Tuple[List[Dict[str, Any]], SearchQuery]:
        """Busca h√≠brida com log da query"""
        
        start_time = time.time()
        
        # Usar pesos padr√£o se n√£o especificados
        if semantic_weight is None:
            semantic_weight = self.semantic_weight
        if bm25_weight is None:
            bm25_weight = self.bm25_weight
        
        # Normalizar pesos
        total_weight = semantic_weight + bm25_weight
        semantic_weight = semantic_weight / total_weight
        bm25_weight = bm25_weight / total_weight
        
        # Buscar com ambos os m√©todos
        embedding_start = time.time()
        semantic_results = self.vector_search.search(query, k * 2)  # Buscar mais para combinar
        embedding_duration = int((time.time() - embedding_start) * 1000)
        
        bm25_results = self.bm25_search.search(query, k * 2)
        
        # Combinar resultados
        combined_results = self._combine_results(
            semantic_results, 
            bm25_results, 
            semantic_weight, 
            bm25_weight
        )
        
        # Limitar ao n√∫mero solicitado
        top_results = combined_results[:k]
        
        search_duration = int((time.time() - start_time) * 1000)
        
        # Log da query
        search_query = SearchQuery.objects.create(
            user=user,
            query_text=query,
            search_type='hybrid',
            semantic_weight=semantic_weight,
            bm25_weight=bm25_weight,
            top_k=k,
            results_count=len(top_results),
            results=[
                {
                    'document_id': result['document_id'],
                    'chunk_id': result['chunk_id'],
                    'combined_score': result['combined_score'],
                    'semantic_score': result.get('semantic_score'),
                    'bm25_score': result.get('bm25_score')
                }
                for result in top_results
            ],
            search_duration_ms=search_duration,
            embedding_duration_ms=embedding_duration
        )
        
        return top_results, search_query
    
    def _combine_results(
        self, 
        semantic_results: List[Dict], 
        bm25_results: List[Dict],
        semantic_weight: float,
        bm25_weight: float
    ) -> List[Dict[str, Any]]:
        """Combina resultados dos dois m√©todos de busca"""
        
        # Criar dicion√°rio para combinar por chunk_id
        combined = {}
        
        # Normalizar scores sem√¢nticos (0-1)
        if semantic_results:
            max_semantic_score = max(result['score'] for result in semantic_results)
            min_semantic_score = min(result['score'] for result in semantic_results)
            score_range = max_semantic_score - min_semantic_score
            
            for result in semantic_results:
                chunk_id = result['chunk_id']
                normalized_score = (result['score'] - min_semantic_score) / score_range if score_range > 0 else 0
                
                combined[chunk_id] = {
                    **result,
                    'semantic_score': normalized_score,
                    'bm25_score': 0.0
                }
        
        # Normalizar scores BM25 (0-1)
        if bm25_results:
            max_bm25_score = max(result['score'] for result in bm25_results)
            min_bm25_score = min(result['score'] for result in bm25_results)
            score_range = max_bm25_score - min_bm25_score
            
            for result in bm25_results:
                chunk_id = result['chunk_id']
                normalized_score = (result['score'] - min_bm25_score) / score_range if score_range > 0 else 0
                
                if chunk_id in combined:
                    combined[chunk_id]['bm25_score'] = normalized_score
                else:
                    combined[chunk_id] = {
                        **result,
                        'semantic_score': 0.0,
                        'bm25_score': normalized_score
                    }
        
        # Calcular score combinado
        for chunk_id, result in combined.items():
            result['combined_score'] = (
                result['semantic_score'] * semantic_weight +
                result['bm25_score'] * bm25_weight
            )
        
        # Ordenar por score combinado
        sorted_results = sorted(
            combined.values(),
            key=lambda x: x['combined_score'],
            reverse=True
        )
        
        return sorted_results